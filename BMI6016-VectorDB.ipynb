{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases\n",
    "\n",
    "## Why Vector Databases?\n",
    "\n",
    "Vector data are high-dimensional and traditional dbs are not built to efficiently store and retrieve vectors. Because of this: Vector DBs are designed to store and retrieve vector data - (duh). \n",
    "\n",
    "## Linear Algebra 101\n",
    "\n",
    "### Vectors\n",
    "\n",
    "<img src=\"https://www.illumination.com/wp-content/uploads/2019/11/DM1_Vector.png\" width=\"250\"/>\n",
    "\n",
    "Vector: **Direction + Magnitude**\n",
    "\n",
    "* collection of numbers\n",
    "\n",
    "* can represent different things (**embedding**)\n",
    "    - language\n",
    "    - images\n",
    "    - audio\n",
    "* High School Cliques Analogy\n",
    "* <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*dyH20eCqb6qTL-gt4nCVzQ.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "* Applications\n",
    "    - text generation\n",
    "    - recommendation systems\n",
    "    - search engines\n",
    "\n",
    "### **Embeddings == Vectors**\n",
    "(but Vector doesn't necessarily mean embedding)\n",
    "\n",
    "### VectorDB\n",
    "* used to store/query these embeddings\n",
    "* arrays of numbers clustered\n",
    "    - relational db: rows/columns\n",
    "    - document db: documents/collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple VectorDB implementation in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install the necessary packages.\n",
    "\n",
    "`langchain` is a framework for using anything related utilizing Large Language Models (LLMs).\n",
    "\n",
    "`sentence-transformers` is required to utilize HuggingFace's Embeddings.\n",
    "\n",
    "`faiss-cpu`: FAISS is a vector DB that will be used in this tutorial.\n",
    "\n",
    "`pypdf`: required package for the 'PDFLoader' we will use - used to read text from PDFs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install torch\n",
    "!pip install faiss-cpu\n",
    "!pip install pypdf\n",
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google Colab, you need to download the sample files shown in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O files.zip https://github.com/KoltonHauck/BMI6016_VectorDB/raw/main/files.zip\n",
    "\n",
    "!unzip files.zip -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import everything we will use.\n",
    "\n",
    "`PyPDFDirectoryLoader` is a 'document loader', which means it processes a folder with .pdfs and extracts the text from them. All of the different loader formats langchain implementations are here: [LangChain Loaders](https://python.langchain.com/docs/integrations/document_loaders)\n",
    "\n",
    "`RecursiveCharacterTextSplitter` is a 'text splitter': it takes in 'document loader' text documents and splits the documents in manageable chunks. Chunking is important for several reasons:\n",
    "1. size limitations of embedding models\n",
    "2. search precision -> when entire docs encoded as single vectors: specificity of embeddings may decrease\n",
    "3. memory efficiency -> processing chunks is computationally cheaper than processing whole documents\n",
    "4. parallel processing -> can process chunks in parallel\n",
    "\n",
    "LangChain text splitters found here: [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "`HuggingFaceEmbeddings`: used to generate the embeddings for the text chunks. (natural language -> vector representation) (The default model selected is [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)) This is just the example used in this example. There are many ways to generate embeddings (just a few):\n",
    "* one hot encoding\n",
    "* word2vec\n",
    "* GloVe\n",
    "* BERT (transformer)\n",
    "\n",
    "`FAISS`: in-memory vector DB used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdfs using PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"files/pdfs/\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "# chunk overlap: some text is shared between adjacent chunks\n",
    "# important for context preservation, continuity in search results, reducing boundary effects\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at first 'text document'\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init embeddings model\n",
    "# text -> vector\n",
    "\n",
    "import torch\n",
    "\n",
    "# Determine if a GPU is available and choose the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "\n",
    "# the length of texts[0].page_content --> 268\n",
    "# embeddings length --> 1024\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is now just a list / array of numbers\n",
    "\n",
    "query_result[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 'texts' is now a point in high-dimensional space (1024D space). Similar texts will be closer together in this high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a Vector Database from these texts using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take several minutes if on CPU\n",
    "# if on cpu, suggest reducing 'texts' being passed in: eg texts[:100]\n",
    "# once created, this is living 'in memory', but can be saved to hard drive if desired\n",
    "\n",
    "vector_db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the VectorDB created, we can now do some pretty cool things with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Similarity Search\n",
    "\n",
    "With the `.similarity_search()` method, we can extract documents (`texts`) from the vector DB that are similar to the query. The query gets embedded, and similar vectors to the query vector are retrieved. Here we are using the `.similarity_search_with_score()` method which is essentially the same, but also provides the `similarity score` between the query and retrieved text. The lower the number, the more similar!\n",
    "\n",
    "The `k` parameter is the number of `texts` to retrieve from the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_search = vector_db.similarity_search_with_score(\"What are some frameworks to assess data quality?\", k=4)\n",
    "\n",
    "sim_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(sim_search):\n",
    "  print(f\"---- Result #{i} | {result[0].metadata['source']} | page {result[0].metadata['page']} | score: {result[1]} ----\")\n",
    "  print(result[0].page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Marginal Relevance (MMR) Search\n",
    "\n",
    "MMR is a search algorithm that attempts to address the limitations of basic similarity search:\n",
    "* redundancy (very similar documents)\n",
    "* coverage (when searching for 'apple': fruit or computer? MMR might return documents relevant to both whereas basic might just return one)\n",
    "* narrow coverage of topic (MMR helps to provide comprehensive view of topic)\n",
    "\n",
    "MMR works by:\n",
    "* calculating relevance scores between query and each document (similar to basic search)\n",
    "* iteratively selecting documents based on similarity to the query AND dissimilarity to already selected documents (can tune with parameter `lambda_mult`)\n",
    "\n",
    "Implemented with `max_marginal_relevance_search` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_mult = 1 (basically basic search) -> takes into no consideration of dissimilarity of already retrieved texts\n",
    "\n",
    "mmr_result_1 = vector_db.max_marginal_relevance_search(\"What are some frameworks to assess data quality?\", k=4, lambda_mult=1)\n",
    "\n",
    "for i, result in enumerate(mmr_result_1):\n",
    "  print(f\"---- Result #{i} | {result.metadata['source']} | page {result.metadata['page']} ----\")\n",
    "  print(result.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_mult = 0 -> wildly takes into consideration of dissimilarity of already retrieved texts\n",
    "\n",
    "mmr_result_0 = vector_db.max_marginal_relevance_search(\"What are some frameworks to assess data quality?\", k=4, lambda_mult=0)\n",
    "\n",
    "for i, result in enumerate(mmr_result_0):\n",
    "  print(f\"---- Result #{i} | {result.metadata['source']} | page {result.metadata['page']} ----\")\n",
    "  print(result.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "RAG is an approach to augment Large Language Models responses by suppling context with the prompt. This helps deal with several issues commonly seen with LLMs:\n",
    "* hallucinations (by supplying context relevant to the query, the model doesn't need to hallucinate - at least not as much - becuase it has the relevant information needed to ***********)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Embedding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) is a great Python NLP package. You can also retrieve embeddings from it!\n",
    "\n",
    "When you initially install spaCy, it comes pre-loaded with a model packed with a bunch of stuff, however, it does not come pre-loaded with the word vectors. So, we downloaded that right after we 'pip installed' spacy: `!python -m spacy download en_core_web_lg`. We load it initially to retrieve the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are loading the \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_emb = nlp.vocab['cheese'].vector # replace cheese\n",
    "\n",
    "print(len(cheese_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't really have a VectorDB with embeddings from two different models / methods. It's like having a dictionary with english and spanish words (but with no translation between them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents, known as a corpus. It increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This helps to adjust for the fact that some words appear more frequently in general. TF-IDF is often used in text mining and information retrieval to weigh and rank words' relevance in documents. You can also use TF-IDF embeddings just like other Embeddings shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Huggingface Models\n",
    "\n",
    "https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Run%20Llama2%20Google%20Colab/Llama_2_updated.ipynb\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python numpy --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install llama-cpp-python\n",
    "!pip install huggingface-hub langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_model_path = hf_hub_download(repo_id=\"TheBloke/Llama-2-7b-Chat-GGUF\", filename=\"llama-2-7b-chat.Q5_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 1024  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=downloaded_model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for just using the llama-cpp package\n",
    "\n",
    "from llama_cpp import Llama\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=downloaded_model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
