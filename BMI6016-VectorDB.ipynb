{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KoltonHauck/BMI6016_VectorDB/blob/main/BMI6016-VectorDB.ipynb)\n",
    "\n",
    "# Vector Databases\n",
    "\n",
    "## Why Vector Databases?\n",
    "\n",
    "Vector data are high-dimensional and traditional dbs are not built to efficiently store and retrieve vectors. Because of this: Vector DBs are designed to store and retrieve vector data - (duh). \n",
    "\n",
    "## Linear Algebra 101\n",
    "\n",
    "### Vectors\n",
    "\n",
    "<img src=\"https://www.illumination.com/wp-content/uploads/2019/11/DM1_Vector.png\" width=\"250\"/>\n",
    "\n",
    "Vector: **Direction + Magnitude**\n",
    "\n",
    "* collection of numbers\n",
    "\n",
    "* can represent different things (**embedding**)\n",
    "    - language\n",
    "    - images\n",
    "    - audio\n",
    "* High School Cliques Analogy\n",
    "* <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*dyH20eCqb6qTL-gt4nCVzQ.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "* Applications\n",
    "    - text generation\n",
    "    - recommendation systems\n",
    "    - search engines\n",
    "\n",
    "### **Embeddings == Vectors**\n",
    "(but Vector doesn't necessarily mean embedding)\n",
    "\n",
    "### VectorDB\n",
    "* used to store/query these embeddings\n",
    "* arrays of numbers clustered\n",
    "    - relational db: rows/columns\n",
    "    - document db: documents/collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple VectorDB implementation in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install the necessary packages.\n",
    "\n",
    "`langchain` is a framework for using anything related utilizing Large Language Models (LLMs).\n",
    "\n",
    "`sentence-transformers` is required to utilize HuggingFace's Embeddings.\n",
    "\n",
    "`faiss-cpu`: FAISS is a vector DB that will be used in this tutorial.\n",
    "\n",
    "`pypdf`: required package for the 'PDFLoader' we will use - used to read text from PDFs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install torch\n",
    "!pip install faiss-cpu\n",
    "!pip install pypdf\n",
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Google Colab, you need to download the sample files shown in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O files.zip https://github.com/KoltonHauck/BMI6016_VectorDB/raw/main/files.zip\n",
    "\n",
    "!unzip files.zip -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import everything we will use.\n",
    "\n",
    "`PyPDFDirectoryLoader` is a 'document loader', which means it processes a folder with .pdfs and extracts the text from them. All of the different loader formats langchain implementations are here: [LangChain Loaders](https://python.langchain.com/docs/integrations/document_loaders)\n",
    "\n",
    "`RecursiveCharacterTextSplitter` is a 'text splitter': it takes in 'document loader' text documents and splits the documents in manageable chunks. Chunking is important for several reasons:\n",
    "1. size limitations of embedding models\n",
    "2. search precision -> when entire docs encoded as single vectors: specificity of embeddings may decrease\n",
    "3. memory efficiency -> processing chunks is computationally cheaper than processing whole documents\n",
    "4. parallel processing -> can process chunks in parallel\n",
    "\n",
    "LangChain text splitters found here: [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "`HuggingFaceEmbeddings`: used to generate the embeddings for the text chunks. (natural language -> vector representation) (The default model selected is [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)) This is just the example used in this example. There are many ways to generate embeddings (just a few):\n",
    "* one hot encoding\n",
    "* word2vec\n",
    "* GloVe\n",
    "* BERT (transformer)\n",
    "\n",
    "`FAISS`: in-memory vector DB used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdfs using PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"files/pdfs/\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "# chunk overlap: some text is shared between adjacent chunks\n",
    "# important for context preservation, continuity in search results, reducing boundary effects\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at first 'text document'\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init embeddings model\n",
    "# text -> vector\n",
    "\n",
    "import torch\n",
    "\n",
    "# Determine if a GPU is available and choose the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "\n",
    "# the length of texts[0].page_content --> 268\n",
    "# embeddings length --> 1024\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is now just a list / array of numbers\n",
    "\n",
    "query_result[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 'texts' is now a point in high-dimensional space (1024D space). Similar texts will be closer together in this high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a Vector Database from these texts using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take several minutes if on CPU\n",
    "# if on cpu, suggest reducing 'texts' being passed in: eg texts[:100]\n",
    "# once created, this is living 'in memory', but can be saved to hard drive if desired\n",
    "\n",
    "vector_db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the VectorDB created, we can now do some pretty cool things with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Similarity Search\n",
    "\n",
    "With the `.similarity_search()` method, we can extract documents (`texts`) from the vector DB that are similar to the query. The query gets embedded, and similar vectors to the query vector are retrieved. Here we are using the `.similarity_search_with_score()` method which is essentially the same, but also provides the `similarity score` between the query and retrieved text. The lower the number, the more similar!\n",
    "\n",
    "The `k` parameter is the number of `texts` to retrieve from the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_search = vector_db.similarity_search_with_score(\"What are some frameworks to assess data quality?\", k=4)\n",
    "\n",
    "sim_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(sim_search):\n",
    "  print(f\"---- Result #{i} | {result[0].metadata['source']} | page {result[0].metadata['page']} | score: {result[1]} ----\")\n",
    "  print(result[0].page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Marginal Relevance (MMR) Search\n",
    "\n",
    "MMR is a search algorithm that attempts to address the limitations of basic similarity search:\n",
    "* redundancy (very similar documents)\n",
    "* coverage (when searching for 'apple': fruit or computer? MMR might return documents relevant to both whereas basic might just return one)\n",
    "* narrow coverage of topic (MMR helps to provide comprehensive view of topic)\n",
    "\n",
    "MMR works by:\n",
    "* calculating relevance scores between query and each document (similar to basic search)\n",
    "* iteratively selecting documents based on similarity to the query AND dissimilarity to already selected documents (can tune with parameter `lambda_mult`)\n",
    "\n",
    "Implemented with `max_marginal_relevance_search` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_mult = 1 (basically basic search) -> takes into no consideration of dissimilarity of already retrieved texts\n",
    "\n",
    "mmr_result_1 = vector_db.max_marginal_relevance_search(\"What are some frameworks to assess data quality?\", k=4, lambda_mult=1)\n",
    "\n",
    "for i, result in enumerate(mmr_result_1):\n",
    "  print(f\"---- Result #{i} | {result.metadata['source']} | page {result.metadata['page']} ----\")\n",
    "  print(result.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_mult = 0 -> wildly takes into consideration of dissimilarity of already retrieved texts\n",
    "\n",
    "mmr_result_0 = vector_db.max_marginal_relevance_search(\"What are some frameworks to assess data quality?\", k=4, lambda_mult=0)\n",
    "\n",
    "for i, result in enumerate(mmr_result_0):\n",
    "  print(f\"---- Result #{i} | {result.metadata['source']} | page {result.metadata['page']} ----\")\n",
    "  print(result.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Embedding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) is a great Python NLP package. You can also retrieve embeddings from it!\n",
    "\n",
    "When you initially install spaCy, it comes pre-loaded with a model packed with a bunch of stuff, however, it does not come pre-loaded with the word vectors. So, we downloaded that right after we 'pip installed' spacy: `!python -m spacy download en_core_web_lg`. We load it initially to retrieve the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are loading the \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_emb = nlp.vocab['cheese'].vector # replace cheese\n",
    "\n",
    "print(len(cheese_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't really have a VectorDB with embeddings from two different models / methods. It's like having a dictionary with english and spanish words (but with no translation between them). So, we can't really combine our `spaCy` embeddings with our `all-mpnet-base-v2` embeddings. We should create two separate indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents, known as a corpus. It increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This helps to adjust for the fact that some words appear more frequently in general. TF-IDF is often used in text mining and information retrieval to weigh and rank words' relevance in documents. You can also use TF-IDF embeddings just like other embeddings shown here.\n",
    "\n",
    "Here we are using `scikit-learn` to use implement `TF-IDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` returns the data in a sparse data format - a way where we can't automatically view it. This just means it's a sparse matrix - a lot of words don't appear in most documents, leading to empty spots in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "\n",
    "LLMs are language models (duh) and are generative models - they create new text. There are other language models:\n",
    "* n-grams\n",
    "* autoencoders\n",
    "* RNNs\n",
    "* and others\n",
    "\n",
    "LLMs are large - trained on vast corpuses of texts. Even though they've been trained on general data (mostly), we can apply `transfer learning` - using a model for a similar task it wasn't trained to perform. This can be highly successful, especially when augmented with prompt fine-tuning, retrieval augmented generation, and few-shot prompting.\n",
    "\n",
    "Here I will show how to download a LLM from HuggingFace, and show how to prompt it via LangChain. I will also show how to connect this model to an vector DB so that we can 'chat' with our files.\n",
    "\n",
    "Then, I will show how to use OpenAI models in the same situation.\n",
    "\n",
    "\n",
    "Other sources used:\n",
    "* [Llama 2 in Colab Example](https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Run%20Llama2%20Google%20Colab/Llama_2_updated.ipynb)\n",
    "* [llamacpp docs in langchain](https://python.langchain.com/docs/integrations/llms/llamacpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2 (quantized)\n",
    "\n",
    "Here we are installing [llama-cpp](https://github.com/ggerganov/llama.cpp) which helps run models locally with minimal set-up. The models must be in a `.gguf` file format.\n",
    "\n",
    "We will download a `llama-2 7b chat` model in `.gguf` file format from HuggingFace. Specifically, we are downloading a quantized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python numpy --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install llama-cpp-python\n",
    "!pip install huggingface-hub langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_model_path = hf_hub_download(repo_id=\"TheBloke/Llama-2-7b-Chat-GGUF\", filename=\"llama-2-7b-chat.Q5_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=downloaded_model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "RAG is an approach to augment Large Language Models responses by suppling context with the prompt. This helps deal with several issues commonly seen with LLMs:\n",
    "* hallucinations (by supplying context relevant to the query, the model has the information it needs)\n",
    "* information overload - don't give all information - just relevant\n",
    "\n",
    "This is of course assuming that the process of retrieving the relevant context is accurate (which is another conversation).\n",
    "\n",
    "In LangChain, this 'retrieval' operation is implemented with LangChain `retrievers` and `chains`. We will use the `RetrievalQAWithSourcesChain`.\n",
    "\n",
    "[LangChain Chains](https://python.langchain.com/docs/modules/chains/)\n",
    "\n",
    "[LangChain Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain also provides an easy way to give `templates` to structure prompts easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "### summaries ###\n",
    "{summaries}\n",
    "### question ###\n",
    "{question}\n",
    "### answer ###\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"summaries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up QA (question-answer) object\n",
    "qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt the model\n",
    "\n",
    "result = qa_chain_with_sources(\"What are some data quality frameworks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what data is returned\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain_with_sources(\"What are some frameworks to assess data quality?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API\n",
    "\n",
    "Here we will do the same thing, but call the OpenAI API rather than download a local model. You do need an API key to run this: [OpenAI API Key](https://platform.openai.com/api-keys) (and will cost very very little money)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"]=\"\"\n",
    "\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",\n",
    "                        temperature=0.5,\n",
    "                        streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=openai_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain_with_sources(\n",
    "    \"What are some frameworks to assess data quality?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Format\n",
    "\n",
    "Lastly, we will show how LLMs can create structured data from unstructured data. GPT-4 has JSON output as a native feature. (Some others might as well), but there are other ways to format output.\n",
    "\n",
    "Here are some examples:\n",
    "* [Format Enforcer - llama-cpp](https://github.com/noamgat/lm-format-enforcer/blob/main/samples/colab_llamacpppython_integration.ipynb)\n",
    "* [LangChain output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)\n",
    "\n",
    "We will just look at GPT-4 in this notebook. Here are some basic examples demonstrating this functionality: [GPT-4 JSON output](https://medium.com/@vishalkalia.er/experimenting-with-gpt-4-turbos-json-mode-a-new-era-in-ai-data-structuring-58d38409f1c7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetically generated chart\n",
    "\n",
    "patient_chart = \"\"\"\n",
    "Patient Name: John Doe\n",
    "DOB: 02/14/1985\n",
    "Gender: Male\n",
    "Allergies: Penicillin, Aspirin\n",
    "Last Visit: 03/10/2023\n",
    "\n",
    "Chief Complaint:\n",
    "Patient presents with severe abdominal pain and recurring headaches over the past two weeks.\n",
    "\n",
    "History of Present Illness:\n",
    "John has been experiencing sharp, intermittent abdominal pain, primarily in the lower right quadrant, with a pain level of 8 out of 10. He reports the pain worsens after meals. Headaches are described as throbbing, occurring bi-weekly, predominantly in the mornings.\n",
    "\n",
    "Past Medical History:\n",
    "- Type 2 Diabetes Mellitus, diagnosed in 2010\n",
    "- Hypertension, under control with medication\n",
    "- Previous appendectomy in 2005\n",
    "\n",
    "Medications:\n",
    "- Metformin 500mg twice daily for diabetes\n",
    "- Lisinopril 10mg once daily for hypertension\n",
    "\n",
    "Family History:\n",
    "- Father with coronary artery disease\n",
    "- Mother with osteoporosis\n",
    "\n",
    "Social History:\n",
    "Non-smoker, consumes alcohol occasionally, works as a software developer, exercises twice a week.\n",
    "\n",
    "Physical Examination:\n",
    "- Vital Signs: BP 130/85, HR 78 bpm, Temp 98.6°F, Resp 16/min\n",
    "- Abdomen: Tenderness noted in the right lower quadrant, no rebound tenderness\n",
    "- Neurological: No focal deficits observed\n",
    "\n",
    "Laboratory Tests:\n",
    "- Complete Blood Count (CBC) normal\n",
    "- Abdominal Ultrasound: Indication of possible cholecystitis\n",
    "\n",
    "Assessment:\n",
    "Suspected acute cholecystitis, secondary to gallstones. The headache likely tension-type, needs further evaluation.\n",
    "\n",
    "Plan:\n",
    "- Admit for observation and surgical consultation for cholecystitis\n",
    "- MRI of the brain to rule out other causes of headaches\n",
    "- Follow up on diabetes and hypertension management\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a clinician.\n",
    "Your task is to extract medical and patient-related entities from the clinical chart text.\n",
    "Identify and structure the output in JSON format, with the following fields: patient information (name, DOB, gender, allergies), visit information (date, chief complaint), medical history (conditions, surgeries), medications, family history, social history, physical examination findings, laboratory tests, assessment, and plan.\n",
    "Each entity should be listed under its respective category with relevant details.\n",
    "\n",
    "### patient chart ###\n",
    "{patient_chart}\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-4\",\n",
    "    prompt=prompt,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
